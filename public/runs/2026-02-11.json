{
  "updatedAt": "2026-02-11 02:30 UTC",
  "tldr": [
    "No net-new episodes detected across all 8 tracked shows in this run window; preserving prior deep notes.",
    "All tracked shows are explicitly included in reader output to maintain scan continuity.",
    "Transcript source policy remains enforced in order: official → platform → YouTube captions → extractor libs → generated.",
    "Each listed show retains structured deep notes with claims, frameworks, counterpoints, experiments, and ignore-list guidance.",
    "latest.json and index.json now point to this 11 Feb 2026 run.",
    "Reader: https://podcast-radar-reader.vercel.app/"
  ],
  "episodes": [
    {
      "show": "Acquired",
      "title": "The NFL (2026 Update)",
      "episodeUrl": "https://www.acquired.fm/episodes/the-nfl",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (10):\n1) The NFL’s enduring edge is control of distribution economics, not just team quality.\n2) Weekly ritual behavior (appointment viewing) is effectively a retention engine.\n3) Governance continuity enables decade-scale strategic consistency.\n4) Revenue is de-risked by multi-rail monetization (rights, sponsorship, ecosystem).\n5) Product truth: rituals + social coordination produce sticky usage.\n6) Scarce inventory windows become premium pricing surfaces.\n7) Ecosystem value compounds when creators/media/adjacent products depend on the core platform.\n8) Execution quality is often institutional (systems), not individual (stars).\n9) Durable moats often look boring operationally but are hard to copy.\n10) What appears as “brand strength” is frequently distribution design in disguise.\n\n3 claims worth remembering:\n- Distribution control can outrun content quality over time.\n- Habit architecture is a moat.\n- Governance design is a growth lever.\n\n2 frameworks:\n- Moat stack = Distribution × Ritual × Governance.\n- Revenue resilience = independent monetization rails count × quality.\n\n2 counterpoints:\n- Rights concentration can suppress experimentation speed.\n- Ritual dependence can increase seasonality sensitivity.\n\n2 experiments (metric + decision):\n- Weekly ritual premium module: metric = repeat WAU on fixed day; decision = scale if +15% vs control in 4 weeks.\n- Sponsored workflow slot: metric = ARPU uplift + retention; decision = keep if ARPU +6% and retention non-negative.\n\nWhat to ignore:\n- Surface-level sports analogies without mechanism translation to your product."
    },
    {
      "show": "Lenny’s Podcast",
      "title": "Dr. Becky — overlap between parenting and difficult adults",
      "episodeUrl": "https://www.lennysnewsletter.com/p/dr-becky-on-the-surprising-overlap",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (10):\n1) Defensive behavior is often a perceived-threat response, not pure disagreement.\n2) Reframing before correction reduces conflict escalation.\n3) Boundaries are more effective when specific, observable, and time-bound.\n4) Validation and accountability are complements, not opposites.\n5) In teams, psychological safety improves information quality and decision speed.\n6) Product copy can trigger identity threat (and tank conversion) if accusatory.\n7) Hard conversations improve when scripted as loops, not one-shot events.\n8) Emotional granularity improves action granularity.\n9) Repeatable communication templates reduce leadership fatigue.\n10) Behavior change sticks when feedback is immediate and concrete.\n\n3 claims:\n- Tone architecture affects conversion quality.\n- Repair loops outperform confrontation spikes.\n- Boundaries are operational tools, not “soft skills.”\n\n2 frameworks:\n- Repair loop: Trigger → Validate → Boundary → Request → Follow-up.\n- Communication quality equation: Clarity × Safety × Specificity.\n\n2 counterpoints:\n- Over-validation can dilute accountability.\n- Scripted communication can feel inauthentic if overused.\n\n2 experiments:\n- Onboarding reframe test: metric = activation uplift + support tickets; decision = ship if +5% activation with no ticket spike.\n- Paywall tone test: metric = conversion + refund rate; decision = keep if conversion up and refunds flat/down.\n\nWhat to ignore:\n- Generic “be empathetic” advice without operational structure."
    },
    {
      "show": "Founders",
      "title": "#411 Tortured Into Greatness: Andre Agassi",
      "episodeUrl": "",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (9):\n1) High output came from disciplined repetition, not sustained motivation.\n2) Identity conflict can coexist with elite execution.\n3) External pressure can create performance but often at psychological cost.\n4) Systems and rituals reduce dependence on mood.\n5) Recovery cycles are as important as effort cycles.\n6) Craft refinement compounds quietly before visible jumps.\n7) Public perception often hides operational grind.\n8) Constraint can sharpen focus but must be bounded.\n9) Personal narrative drives behavior, for better or worse.\n\n3 claims:\n- Process consistency beats motivational spikes.\n- Recovery is part of performance, not a break from it.\n- Identity story influences execution quality.\n\n2 frameworks:\n- Performance stack: Process × Recovery × Feedback.\n- Consistency loop: Commit → Execute → Review → Adjust.\n\n2 counterpoints:\n- Trauma-driven discipline is not a scalable team template.\n- Extreme routines can reduce adaptability.\n\n2 experiments:\n- Weekly ship ritual: metric = experiments shipped/week; decision = keep if >=2/week sustained 6 weeks.\n- Decision journal: metric = decision cycle time + reversal rate; decision = keep if -20% cycle time with flat reversal.\n\nWhat to ignore:\n- Hero mythology without reproducible operating mechanics."
    },
    {
      "show": "20VC",
      "title": "Navan public story + AI support",
      "episodeUrl": "https://thetwentyminutevc.libsyn.com/20vc-from-62bn-market-cap-to-28bn-what-is-not-translating-about-navans-public-story-are-any-public-company-ceos-actually-happy-why-navan-built-its-own-customer-service-ai-and-what-it-could-mean-for-customer-service-ai-with-ariel-cohen",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (11):\n1) Public markets punish narrative gaps faster than private markets.\n2) AI support quality is dominated by context depth and integration, not model brand.\n3) Build-vs-buy is fundamentally a data-loop question.\n4) Segment-level proof matters more than blended averages post-IPO.\n5) Support AI can be a monetization product, not just a cost center.\n6) Internal tooling advantage is compounding if iteration loops are short.\n7) Communication to market must map roadmap to concrete margin/revenue outcomes.\n8) Generic bots fail in high-stakes, exception-heavy workflows.\n9) Customer support is increasingly product surface, not back-office.\n10) Trust in automation is earned via reliability and escalation quality.\n11) Narrative without instrumentation is fragile.\n\n3 claims:\n- Context is the product in AI support.\n- Story lag is expensive in public markets.\n- Integration depth determines defensibility.\n\n2 frameworks:\n- Build-vs-buy score = data leverage + integration depth + iteration speed - maintenance drag.\n- Story-to-market fit = narrative clarity × metric credibility.\n\n2 counterpoints:\n- In-house AI increases platform complexity burden.\n- Public pressure can force short-term optimization.\n\n2 experiments:\n- Premium AI support lane: metric = TTR, NPS, upsell; decision = scale if NPS +8 and upsell +3pp.\n- Context-depth ablation: metric = first-contact resolution; decision = ship only if +20% vs generic baseline.\n\nWhat to ignore:\n- Valuation chatter detached from customer outcomes."
    },
    {
      "show": "Invest Like the Best",
      "title": "Ben Horowitz — Backing America’s Future",
      "episodeUrl": "https://colossus.com/episode/backing-americas-future/",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (9):\n1) Enduring value accrues to conviction paired with distribution and timing.\n2) Platform-level control points matter more than incremental features.\n3) Category narrative shapes adoption speed and talent gravity.\n4) Strategic focus outperforms broad but shallow expansion.\n5) Concentrated bets require superior decision hygiene.\n6) Institutional relationships can become compounding distribution channels.\n7) Moat quality is often tested at stress points, not growth periods.\n8) Allocation discipline is an underrated edge.\n9) Optionality is useful only with execution bandwidth.\n\n3 claims:\n- Clarity compounds faster than breadth.\n- Distribution + policy context can reshape outcome ranges.\n- Concentration can outperform with true edge.\n\n2 frameworks:\n- Edge stack = insight × access × execution speed.\n- Focus test = strategic priority count vs resource reality.\n\n2 counterpoints:\n- Concentration increases downside volatility.\n- Narrative confidence can hide weak fundamentals.\n\n2 experiments:\n- One-module domination sprint: metric = attach rate + retention; decision = continue if attach +25% in 6 weeks.\n- Positioning stress test: metric = close-rate delta in sales calls; decision = adopt if +10% absolute.\n\nWhat to ignore:\n- Macro theater that doesn’t cash out into product choices."
    },
    {
      "show": "Training Data (Sequoia)",
      "title": "Vertical SaaS in an AGI World",
      "episodeUrl": "https://feeds.megaphone.fm/trainingdata",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (12):\n1) Adoption bottleneck is trust/control, not pure model capability.\n2) Regulated workflows require auditable actions + override controls.\n3) BPO replacement is a practical AI wedge with clear ROI.\n4) Buyers pay to reduce variance and compliance risk.\n5) Explainability is commercial infrastructure, not just legal checkbox.\n6) Domain-specific evals outperform generic benchmarks for buying decisions.\n7) Workflow insertion point quality predicts retained usage.\n8) Reliability SLAs should be productized and sold.\n9) AI module pricing should map to economic outcomes, not token usage.\n10) Human-in-the-loop fallback quality drives trust retention.\n11) Integration surface area determines switching costs.\n12) Vertical depth can be a feature and a TAM constraint simultaneously.\n\n3 claims:\n- Reliability beats novelty in verticals.\n- Auditability is a value prop.\n- Outcome ownership unlocks premium pricing.\n\n2 frameworks:\n- Trust stack = Reliability + Explainability + Control.\n- Value equation = (time saved + error reduced + variance reduced) per workflow.\n\n2 counterpoints:\n- Over-governance can kill UX velocity.\n- Vertical depth can slow expansion if architecture isn’t extensible.\n\n2 experiments:\n- Regulated workflow pilot: metric = cycle time + error rate; decision = scale if -25% time with error <= baseline.\n- Audit add-on pricing: metric = attach + renewal delta; decision = keep if attach >18% and renewal +4pp.\n\nWhat to ignore:\n- AGI timeline debates not tied to buyer behavior now."
    },
    {
      "show": "Y Combinator Startup Podcast",
      "title": "OpenClaw and the future of personal agents",
      "episodeUrl": "https://podcasters.spotify.com/pod/show/ycombinator/episodes/OpenClaw-And-The-Future-Of-Personal-AI-Agents-e3eonov",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (9):\n1) Local-first + message-native interfaces cut user friction dramatically.\n2) Agent utility scales with memory continuity and tool depth.\n3) Outcome packaging converts better than generic “AI access.”\n4) Multi-surface availability (chat/web/ops) increases stickiness.\n5) Permission architecture directly impacts trust and adoption.\n6) Operational reliability beats clever demos.\n7) Users prefer integrations with existing workflows over new destination apps.\n8) Agent personality/tone influences engagement quality.\n9) Distribution through current channels compresses go-to-market.\n\n3 claims:\n- Existing workflow insertion beats net-new behavior creation.\n- Utility is mostly context + execution reliability.\n- Outcome pricing is structurally stronger.\n\n2 frameworks:\n- Agent utility = context memory × tool execution × reliability.\n- Friction model = new behavior cost vs embedded behavior leverage.\n\n2 counterpoints:\n- Broad permissions increase blast radius risk.\n- Message-first UX can hide complexity/constraints.\n\n2 experiments:\n- Outcome-pack pricing: metric = trial→paid conversion; decision = +4pp to keep.\n- Message-native onboarding: metric = day-7 retention; decision = +12% vs app-first path.\n\nWhat to ignore:\n- Agent hype without measurable user outcomes."
    },
    {
      "show": "Redpoint/Unsupervised Learning",
      "title": "Ex-OpenAI researcher on RL scaling limits",
      "episodeUrl": "https://unsupervised-learning.simplecast.com/episodes/ep-81-ex-openai-researcher-on-why-he-left-his-honest-agi-timeline-the-limits-of-scaling-rl-v8n6gexx-fZAknHYv",
      "transcriptStatus": "no-new",
      "notes": "Concrete notes (10):\n1) Scaling remains powerful but doesn’t solve all generalization gaps.\n2) Eval quality is now a gating factor for practical reliability.\n3) Continual learning remains an unresolved bottleneck.\n4) Benchmarks can overstate production readiness.\n5) Real-world constraints (latency/cost/failure handling) dominate deployment value.\n6) Reliability in narrow domains often beats broad capability claims.\n7) Objective misspecification remains a recurring failure mode.\n8) Data quality and feedback loops can outperform raw parameter growth.\n9) System design still matters as much as model choice.\n10) Org-level experimentation speed is strategic.\n\n3 claims:\n- Benchmark progress is necessary but not sufficient.\n- Eval rigor is becoming a moat.\n- Narrow reliability beats broad uncertainty in monetization contexts.\n\n2 frameworks:\n- Capability equation = Scale × Data quality × Eval rigor.\n- Deployment fitness = reliability × latency × cost × failure recovery.\n\n2 counterpoints:\n- Underestimating scaling may leave upside on table.\n- Over-emphasis on caution can slow product advantage capture.\n\n2 experiments:\n- Reliability-gated routing: metric = variance in task success; decision = keep if variance -30%.\n- Eval harness upgrade: metric = escaped failures; decision = ship if -40% escapes.\n\nWhat to ignore:\n- Timeline tribalism without deployment constraints context."
    }
  ],
  "rabbitHoles": [
    "How to engineer ritual usage into SMB finance workflows without adding friction?",
    "What pricing architecture best monetizes auditability (tier vs add-on vs usage)?",
    "At what proprietary-data threshold does support AI shift from buy to build?",
    "Which leading indicators predict ARPU lift earliest for micro-modules?",
    "How to institutionalize stop/scale rules so experiments don’t drift?"
  ]
}
